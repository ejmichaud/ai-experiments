{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from foresight.ei import ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = Path().absolute()\n",
    "dataset_path = dir_path.parent / \"data/mnist.pkl.gz\"\n",
    "if not dataset_path.exists():\n",
    "    print('Downloading dataset with curl ...')\n",
    "    if not dataset_path.parent.exists():\n",
    "        os.mkdir(dataset_path.parent)\n",
    "    url = 'http://ericjmichaud.com/downloads/mnist.pkl.gz'\n",
    "    os.system('curl -L {} -o {}'.format(url, dataset_path))\n",
    "print('Download failed') if not dataset_path.exists() else print('Dataset acquired')\n",
    "f = gzip.open(dataset_path, 'rb')\n",
    "mnist = pickle.load(f)\n",
    "f.close()\n",
    "print('Loaded data to variable `mnist`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# dtype = torch.cuda.float if torch.cuda.is_available() else torch.float\n",
    "dtype = torch.float32\n",
    "torch.set_default_dtype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    \"\"\"MNIST Digits Dataset.\"\"\"\n",
    "    def __init__(self, data, transform=None):\n",
    "        \"\"\"We save the dataset images as torch.tensor since saving \n",
    "        the dataset in memory inside a `Dataset` object as a \n",
    "        python list or a numpy array causes a multiprocessiing-related \n",
    "        memory leak.\"\"\"\n",
    "        self.images, self.labels = zip(*data)\n",
    "        self.images = torch.from_numpy(np.array(self.images)).to(dtype)\n",
    "        self.labels = torch.tensor(np.argmax(self.labels, axis=1)).to(torch.long)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.images[idx], self.labels[idx]\n",
    "        if self.transform:\n",
    "            image, label = self.transform((image, label))\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(nn.Module):\n",
    "    \"\"\"Single-layer softmax network.\"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(SoftmaxRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_in, n_out, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.linear(x), dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Hidden Layer Softmax Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MNISTDataset(mnist[:60000])\n",
    "test_data = MNISTDataset(mnist[60000:])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_REALIZATIONS = 10\n",
    "\n",
    "# --- ALL DATA ---\n",
    "graph_data = []\n",
    "\n",
    "\n",
    "for realization in range(NUM_REALIZATIONS):\n",
    "    print(\"Starting realization {} of {}\".format(realization, NUM_REALIZATIONS))\n",
    "    \n",
    "    model = SoftmaxRegression(28*28, 10).to(device)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    # --- DATA TO COLLECT FOR EACH REALIZATION ---\n",
    "    num_batches_data = []\n",
    "    eis_data = []\n",
    "    losses_data = [] #[(train, test), (train, test), ...]\n",
    "    accuracies_data = []\n",
    "\n",
    "    def update_metrics():\n",
    "        with torch.no_grad():\n",
    "            input = next(iter(test_loader))[0].to(device)\n",
    "            EI = ei(model, input=input, device=device)\n",
    "\n",
    "        outof = 0\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, labels in islice(test_loader, 0, 500): # 500 batches of 20 samples\n",
    "                output = model(x.to(device))\n",
    "                loss += loss_fn(output, labels.to(device)).item()\n",
    "                _, pred = torch.max(output, 1)\n",
    "                outof += len(labels)\n",
    "        test_loss = loss / outof\n",
    "\n",
    "        outof = 0\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, labels in islice(train_loader, 0, 500): # 500 batches of 20 samples\n",
    "                output = model(x.to(device))\n",
    "                loss += loss_fn(output, labels.to(device)).item()\n",
    "                _, pred = torch.max(output, 1)\n",
    "                outof += len(labels)\n",
    "        training_loss = loss / outof\n",
    "\n",
    "        outof = 0\n",
    "        accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for x, labels in test_loader:\n",
    "                output = model(x.to(device))\n",
    "                _, pred = torch.max(output, 1)\n",
    "                accuracy += (pred == labels.to(device)).sum().item()\n",
    "                outof += len(labels)\n",
    "        accuracy = accuracy / outof\n",
    "\n",
    "        num_batches_data.append(num_batches)\n",
    "        eis_data.append(EI)\n",
    "        losses_data.append((training_loss, test_loss))\n",
    "        accuracies_data.append(accuracy)\n",
    "\n",
    "        if num_batches % 1500 == 0:\n",
    "            print(\"Epoch: {:3d} | EI: {:2.3f} | Accuracy: {:0.3f}\".format(epoch, EI, accuracy))\n",
    "\n",
    "    # --- TRAIN ---\n",
    "\n",
    "    num_batches = 0\n",
    "    for epoch in range(80):\n",
    "        for sample, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(sample.to(device)), target.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            num_batches += 1\n",
    "            if num_batches % 100 == 0:\n",
    "                update_metrics()\n",
    "    print(\"Appending data for realization {}\".format(realization))\n",
    "    graph_data.append((num_batches_data, eis_data, losses_data, accuracies_data))\n",
    "    \n",
    "import pickle\n",
    "with open(\"plots/graph_data_many.pkl\", \"wb\") as f:\n",
    "    pickle.dump(graph_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
